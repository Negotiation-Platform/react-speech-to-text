[{"D:\\react-speech-to-text\\src\\index.js":"1","D:\\react-speech-to-text\\src\\App.tsx":"2","D:\\react-speech-to-text\\src\\Hooks\\index.tsx":"3","D:\\react-speech-to-text\\src\\Hooks\\recorderHelpers.js":"4","D:\\react-speech-to-text\\src\\Hooks\\recorder.js":"5"},{"size":153,"mtime":1623252451244,"results":"6","hashOfConfig":"7"},{"size":1444,"mtime":1623255821148,"results":"8","hashOfConfig":"7"},{"size":12189,"mtime":1623259734313,"results":"9","hashOfConfig":"7"},{"size":1401,"mtime":1623252451244,"results":"10","hashOfConfig":"7"},{"size":8788,"mtime":1623252451244,"results":"11","hashOfConfig":"7"},{"filePath":"12","messages":"13","errorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":"14"},"zpb6un",{"filePath":"15","messages":"16","errorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":"17"},{"filePath":"18","messages":"19","errorCount":0,"warningCount":2,"fixableErrorCount":0,"fixableWarningCount":0,"source":"20","usedDeprecatedRules":"17"},{"filePath":"21","messages":"22","errorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":"14"},{"filePath":"23","messages":"24","errorCount":0,"warningCount":3,"fixableErrorCount":0,"fixableWarningCount":0,"source":"25","usedDeprecatedRules":"26"},"D:\\react-speech-to-text\\src\\index.js",[],["27","28"],"D:\\react-speech-to-text\\src\\App.tsx",[],["29","30"],"D:\\react-speech-to-text\\src\\Hooks\\index.tsx",["31","32"],"import { useState, useEffect, useRef } from 'react';\r\nimport Hark from 'hark';\r\nimport { startRecording, stopRecording } from './recorderHelpers';\r\n\r\n// https://cloud.google.com/speech-to-text/docs/reference/rest/v1/RecognitionConfig\r\nimport { GoogleCloudRecognitionConfig } from './GoogleCloudRecognitionConfig';\r\n\r\n// https://developer.mozilla.org/en-US/docs/Web/API/SpeechRecognition\r\nexport interface SpeechRecognitionProperties {\r\n  // continuous: do not pass continuous here, instead pass it as a param to the hook\r\n  grammars?: SpeechGrammarList;\r\n  interimResults?: boolean;\r\n  lang?: string;\r\n  maxAlternatives?: number;\r\n}\r\n\r\nconst isEdgeChromium = navigator.userAgent.indexOf('Edg/') !== -1;\r\n\r\ninterface BraveNavigator extends Navigator {\r\n  brave: {\r\n    isBrave: () => Promise<boolean>;\r\n  };\r\n}\r\n\r\nconst AudioContext = window.AudioContext || (window as any).webkitAudioContext;\r\n\r\nconst SpeechRecognition =\r\n  window.SpeechRecognition || (window as any).webkitSpeechRecognition;\r\n\r\nlet recognition: SpeechRecognition | null;\r\n\r\n// Set recognition back to null for brave browser due to promise resolving\r\n// after the conditional on line 31\r\nif ((navigator as BraveNavigator).brave) {\r\n  (navigator as BraveNavigator).brave.isBrave().then((bool) => {\r\n    if (bool) recognition = null;\r\n  });\r\n}\r\n\r\n// Chromium browsers will have the SpeechRecognition method\r\n// but do not implement the functionality due to google wanting ðŸ’°\r\n// this covers new Edge and line 22 covers Brave, the two most popular non-chrome chromium browsers\r\nif (!isEdgeChromium && SpeechRecognition) {\r\n  recognition = new SpeechRecognition();\r\n}\r\n\r\nexport interface UseSpeechToTextTypes {\r\n  continuous?: boolean;\r\n  crossBrowser?: boolean;\r\n  googleApiKey?: string;\r\n  googleCloudRecognitionConfig?: GoogleCloudRecognitionConfig;\r\n  onStartSpeaking?: () => any;\r\n  onStoppedSpeaking?: () => any;\r\n  speechRecognitionProperties?: SpeechRecognitionProperties;\r\n  timeout?: number;\r\n  useOnlyGoogleCloud?: boolean;\r\n}\r\n\r\nexport default function useSpeechToText({\r\n  continuous,\r\n  crossBrowser,\r\n  googleApiKey,\r\n  googleCloudRecognitionConfig,\r\n  onStartSpeaking,\r\n  onStoppedSpeaking,\r\n  speechRecognitionProperties,\r\n  timeout,\r\n  useOnlyGoogleCloud = false\r\n}: UseSpeechToTextTypes) {\r\n  const [isRecording, setIsRecording] = useState(false);\r\n\r\n  const audioContextRef = useRef<AudioContext>();\r\n\r\n  const [results, setResults] = useState<string[]>([]);\r\n  const [interimResult, setInterimResult] = useState<string | undefined>();\r\n  const [error, setError] = useState('');\r\n\r\n  const timeoutId = useRef<number>();\r\n  const mediaStream = useRef<MediaStream>();\r\n\r\n  useEffect(() => {\r\n    if (!crossBrowser && !recognition) {\r\n      setError('Speech Recognition API is only available on Chrome');\r\n    }\r\n\r\n    if (!navigator?.mediaDevices?.getUserMedia) {\r\n      setError('getUserMedia is not supported on this device/browser :(');\r\n    }\r\n\r\n    if ((crossBrowser || useOnlyGoogleCloud) && !googleApiKey) {\r\n      console.error(\r\n        'No google cloud API key was passed, google API will not be able to process speech'\r\n      );\r\n    }\r\n\r\n    if (!audioContextRef.current) {\r\n      audioContextRef.current = new AudioContext();\r\n    }\r\n  }, []);\r\n\r\n  // Chrome Speech Recognition API:\r\n  // Only supported on Chrome browsers\r\n  const chromeSpeechRecognition = () => {\r\n    if (recognition) {\r\n      // Continuous recording after stopped speaking event\r\n      if (continuous) recognition.continuous = true;\r\n\r\n      const { grammars, interimResults, lang, maxAlternatives } =\r\n        speechRecognitionProperties || {};\r\n\r\n      if (grammars) recognition.grammars = grammars;\r\n      if (lang) recognition.lang = lang;\r\n\t  \r\n\t  console.log('ChromeSpeechRecognition')\r\n\r\n      recognition.interimResults = interimResults || false;\r\n      recognition.maxAlternatives = maxAlternatives || 1;\r\n\r\n      // start recognition\r\n      recognition.start();\r\n\r\n      // speech successfully translated into text\r\n      recognition.onresult = (e) => {\r\n        const result = e.results[e.results.length - 1];\r\n        const { transcript } = result[0];\r\n\t\t\r\n\t\tconsole.log('interimSetting', interimResults)\r\n        // Allows for realtime speech result UI feedback\r\n        if (interimResults) {\r\n          if (result.isFinal) {\r\n            setInterimResult(undefined);\r\n            setResults((prevResults) => [...prevResults, transcript]);\r\n          } else {\r\n            let concatTranscripts = '';\r\n\r\n            // If continuous: e.results will include previous speech results: need to start loop at the current event resultIndex for proper concatenation\r\n            for (let i = e.resultIndex; i < e.results.length; i++) {\r\n              concatTranscripts += e.results[i][0].transcript;\r\n            }\r\n\r\n            setInterimResult(concatTranscripts);\r\n\t\t\tconsole.log('interimResult', concatTranscripts);\r\n          }\r\n        } else {\r\n          setResults((prevResults) => [...prevResults, transcript]);\r\n        }\r\n      };\r\n\r\n      recognition.onaudiostart = () => setIsRecording(true);\r\n\r\n      // Audio stopped recording or timed out.\r\n      // Chrome speech auto times-out if no speech after a while\r\n      recognition.onend = () => {\r\n        setIsRecording(false);\r\n      };\r\n    }\r\n  };\r\n\r\n  const startSpeechToText = async () => {\r\n    if (!useOnlyGoogleCloud && recognition) {\r\n      chromeSpeechRecognition();\r\n      return;\r\n    }\r\n\r\n    if (!crossBrowser && !useOnlyGoogleCloud) {\r\n      return;\r\n    }\r\n\r\n    // Resume audio context due to google auto play policy\r\n    // https://developers.google.com/web/updates/2017/09/autoplay-policy-changes#webaudio\r\n    if (audioContextRef.current?.state === 'suspended') {\r\n      audioContextRef.current?.resume();\r\n    }\r\n\r\n    const stream = await startRecording({\r\n      errHandler: () => setError('Microphone permission was denied'),\r\n      audioContext: audioContextRef.current as AudioContext\r\n    });\r\n\r\n    // Stop recording if timeout\r\n    if (timeout) {\r\n      handleRecordingTimeout();\r\n    }\r\n\r\n    // stop previous mediaStream track if exists\r\n    if (mediaStream.current) {\r\n      mediaStream.current.getAudioTracks()[0].stop();\r\n    }\r\n\r\n    // Clones stream to fix hark bug on Safari\r\n    mediaStream.current = stream.clone();\r\n\r\n    const speechEvents = Hark(mediaStream.current, {\r\n      audioContext: audioContextRef.current as AudioContext\r\n    });\r\n\r\n    speechEvents.on('speaking', () => {\r\n      if (onStartSpeaking) onStartSpeaking();\r\n\r\n      // Clear previous recording timeout on every speech event\r\n      clearTimeout(timeoutId.current);\r\n    });\r\n\r\n    speechEvents.on('stopped_speaking', () => {\r\n      if (onStoppedSpeaking) onStoppedSpeaking();\r\n\r\n      setIsRecording(false);\r\n      mediaStream.current?.getAudioTracks()[0].stop();\r\n\r\n      // Stops current recording and sends audio string to google cloud.\r\n      // recording will start again after google cloud api\r\n      // call if `continuous` prop is true. Until the api result\r\n      // returns, technically the microphone is not being captured again\r\n      stopRecording({\r\n        exportWAV: true,\r\n        wavCallback: (blob) =>\r\n          handleBlobToBase64({ blob, continuous: continuous || false })\r\n      });\r\n    });\r\n\r\n    setIsRecording(true);\r\n  };\r\n\r\n  const stopSpeechToText = () => {\r\n    if (recognition && !useOnlyGoogleCloud) {\r\n      recognition.stop();\r\n    } else {\r\n      setIsRecording(false);\r\n      mediaStream.current?.getAudioTracks()[0].stop();\r\n      stopRecording({\r\n        exportWAV: true,\r\n        wavCallback: (blob) => handleBlobToBase64({ blob, continuous: false })\r\n      });\r\n    }\r\n  };\r\n\r\n  const handleRecordingTimeout = () => {\r\n    timeoutId.current = window.setTimeout(() => {\r\n      setIsRecording(false);\r\n      mediaStream.current?.getAudioTracks()[0].stop();\r\n      stopRecording({ exportWAV: false });\r\n    }, timeout);\r\n  };\r\n\r\n  const handleBlobToBase64 = ({\r\n    blob,\r\n    continuous\r\n  }: {\r\n    blob: Blob;\r\n    continuous: boolean;\r\n  }) => {\r\n    const reader = new FileReader();\r\n    reader.readAsDataURL(blob);\r\n\r\n    reader.onloadend = async () => {\r\n      const base64data = reader.result as string;\r\n\r\n      let sampleRate = audioContextRef.current?.sampleRate\r\n\r\n      // Google only accepts max 48000 sample rate: if\r\n      // greater recorder js will down-sample to 48000\r\n      if (sampleRate && sampleRate > 48000) {\r\n        sampleRate = 48000;\r\n      }\r\n\r\n      const audio = { content: '' };\r\n\r\n      const negotiationPhrases = [\r\n        'all the remaining',\r\n        \"that's it\",\r\n        'You take',\r\n        'I want everything',\r\n        'I would like to',\r\n        'I would like',\r\n        'I want to',\r\n        'I need to',\r\n        'Rest is yours',\r\n        'You can have the rest',\r\n        'I offer',\r\n        'I accept',\r\n        'You give me',\r\n        'All remaining',\r\n        'I agree',\r\n        'you can',\r\n        'I can give',\r\n        'I want'\r\n      ];\r\n\r\n      const domainKeywords = [\t\t\r\n        'one apple',\r\n        'two apple',\r\n        'two apples',\r\n        'three apple',\r\n        'three apples',\r\n        'four apple',\r\n        'four apples',\r\n        'one banana',\r\n        'two banana',\r\n        'two bananas',\r\n        'three banana',\r\n        'three bananas',\r\n        'four banana',\r\n        'four bananas',\r\n        'one orange',\r\n        'two orange',\r\n        'two oranges',\r\n        'three orange',\r\n        'three oranges',\r\n        'four orange',\r\n        'four oranges',\r\n        'one watermelon',\r\n        'two watermelon',\r\n        'two watermelons',\r\n        'three watermelon',\r\n        'three watermelons',\r\n        'four watermelon',\r\n        'four watermelons',\r\n        'all apples',\r\n        'all oranges',\r\n        'all bananas',\r\n        'all watermelons',\r\n        'all of apples',\r\n        'all of oranges',\r\n        'all of bananas',\r\n        'all of watermelons',\r\n        'all of the apples',\r\n        'all of the oranges',\r\n        'all of the bananas',\r\n        'all of the watermelons',\r\n        'zero apple',\r\n        'zero orange',\r\n        'zero banana',\r\n        'zero watermelon',\r\n        'all of them'\r\n      ];\r\n\t  \r\n\t  const rawKeywords = [\r\n\t\t'apple',\r\n\t\t'apples',\r\n\t\t'banana',\r\n\t\t'bananas',\r\n\t\t'orange',\r\n\t\t'oranges',\t\t\r\n\t\t'watermelon',\r\n\t\t'watermelons'\r\n\t  ]\r\n\t  \r\n\t  const rawKeywordsContextsElement = {\r\n\t\tphrases: rawKeywords,\r\n\t\tboost: 40.0\r\n\t  };\t  \r\n\r\n      const negoSpeechContextsElement = {\r\n        phrases: negotiationPhrases,\r\n        boost: 80.0\r\n      };\r\n\r\n      const domainSpeechContextsElement = {\r\n        phrases: domainKeywords,\r\n        boost: 100.0\r\n      };\r\n\r\n      const speechContexts = [\r\n        negoSpeechContextsElement,\r\n        domainSpeechContextsElement\r\n      ];\r\n\r\n      const config: GoogleCloudRecognitionConfig = {\r\n        encoding: 'LINEAR16',\r\n        languageCode: 'en-US',\r\n\t\tmaxAlternatives: 10,\r\n\t\tuseEnhanced: true,\r\n        sampleRateHertz: sampleRate,\r\n        speechContexts: speechContexts,\r\n        ...googleCloudRecognitionConfig\r\n      };\r\n\r\n      const data = {\r\n        config,\r\n        audio\r\n      };\r\n\r\n      // Gets raw base 64 string data\r\n      audio.content = base64data.substr(base64data.indexOf(',') + 1);\r\n\r\n      const googleCloudRes = await fetch(\r\n        `https://speech.googleapis.com/v1p1beta1/speech:recognize?key=${googleApiKey}`,\r\n        {\r\n          method: 'POST',\r\n          body: JSON.stringify(data)\r\n        }\r\n      );\r\n\r\n      const googleCloudJson = await googleCloudRes.json();\r\n\r\n      // Update results state with transcribed text\r\n      if (googleCloudJson.results?.length > 0) {\r\n        setResults((prevResults) => [\r\n          ...prevResults,\r\n\t\t  googleCloudJson.results[0].alternatives.reduce(function(prev:any, current:any) {\r\n\t\t\t\treturn (prev.confidence > current.confidence) ? prev : current\r\n\t\t  }).transcript\r\n          // googleCloudJson.results[0].alternatives[0].transcript\r\n        ]);\r\n      }\r\n\r\n      if (continuous) {\r\n        startSpeechToText();\r\n      }\r\n    };\r\n  };\r\n\r\n  return {\r\n    error,\r\n    interimResult,\r\n    isRecording,\r\n    results,\r\n    startSpeechToText,\r\n    stopSpeechToText\r\n  };\r\n}\r\n","D:\\react-speech-to-text\\src\\Hooks\\recorderHelpers.js",[],"D:\\react-speech-to-text\\src\\Hooks\\recorder.js",["33","34","35"],"import InlineWorker from 'inline-worker';\r\n\r\nexport class Recorder {\r\n  constructor(source, cfg) {\r\n    this.config = {\r\n      bufferLen: 4096,\r\n      numChannels: 1,\r\n      mimeType: 'audio/wav',\r\n      ...cfg\r\n    };\r\n    this.recording = false;\r\n    this.callbacks = {\r\n      getBuffer: [],\r\n      exportWAV: []\r\n    };\r\n    this.context = source.context;\r\n    this.node = (\r\n      this.context.createScriptProcessor || this.context.createJavaScriptNode\r\n    ).call(\r\n      this.context,\r\n      this.config.bufferLen,\r\n      this.config.numChannels,\r\n      this.config.numChannels\r\n    );\r\n\r\n    this.node.onaudioprocess = (e) => {\r\n      if (!this.recording) return;\r\n\r\n      var buffer = [];\r\n      for (var channel = 0; channel < this.config.numChannels; channel++) {\r\n        buffer.push(e.inputBuffer.getChannelData(channel));\r\n      }\r\n      this.worker.postMessage({\r\n        command: 'record',\r\n        buffer: buffer\r\n      });\r\n    };\r\n\r\n    source.connect(this.node);\r\n    this.node.connect(this.context.destination); //this should not be necessary\r\n\r\n    let self = {};\r\n    this.worker = new InlineWorker(function () {\r\n      let recLength = 0,\r\n        recBuffers = [],\r\n        sampleRate,\r\n        numChannels;\r\n\r\n      this.onmessage = function (e) {\r\n        switch (e.data.command) {\r\n          case 'init':\r\n            init(e.data.config);\r\n            break;\r\n          case 'record':\r\n            record(e.data.buffer);\r\n            break;\r\n          case 'exportWAV':\r\n            exportWAV(e.data.type);\r\n            break;\r\n          case 'getBuffer':\r\n            getBuffer();\r\n            break;\r\n          case 'clear':\r\n            clear();\r\n            break;\r\n        }\r\n      };\r\n\r\n      let newSampleRate;\r\n\r\n      function init(config) {\r\n        sampleRate = config.sampleRate;\r\n        numChannels = config.numChannels;\r\n        initBuffers();\r\n\r\n        if (sampleRate > 48000) {\r\n          newSampleRate = 48000;\r\n        } else {\r\n          newSampleRate = sampleRate;\r\n        }\r\n      }\r\n\r\n      function record(inputBuffer) {\r\n        for (var channel = 0; channel < numChannels; channel++) {\r\n          recBuffers[channel].push(inputBuffer[channel]);\r\n        }\r\n        recLength += inputBuffer[0].length;\r\n      }\r\n\r\n      function exportWAV(type) {\r\n        let buffers = [];\r\n        for (let channel = 0; channel < numChannels; channel++) {\r\n          buffers.push(mergeBuffers(recBuffers[channel], recLength));\r\n        }\r\n        let interleaved;\r\n        if (numChannels === 2) {\r\n          interleaved = interleave(buffers[0], buffers[1]);\r\n        } else {\r\n          interleaved = buffers[0];\r\n        }\r\n\r\n        // converts sample rate to 48000 if higher than 48000\r\n        let downSampledBuffer = downSampleBuffer(interleaved, newSampleRate);\r\n\r\n        let dataview = encodeWAV(downSampledBuffer);\r\n        let audioBlob = new Blob([dataview], { type: type });\r\n\r\n        this.postMessage({ command: 'exportWAV', data: audioBlob });\r\n      }\r\n\r\n      function getBuffer() {\r\n        let buffers = [];\r\n        for (let channel = 0; channel < numChannels; channel++) {\r\n          buffers.push(mergeBuffers(recBuffers[channel], recLength));\r\n        }\r\n        this.postMessage({ command: 'getBuffer', data: buffers });\r\n      }\r\n\r\n      function clear() {\r\n        recLength = 0;\r\n        recBuffers = [];\r\n        initBuffers();\r\n      }\r\n\r\n      function initBuffers() {\r\n        for (let channel = 0; channel < numChannels; channel++) {\r\n          recBuffers[channel] = [];\r\n        }\r\n      }\r\n\r\n      function mergeBuffers(recBuffers, recLength) {\r\n        let result = new Float32Array(recLength);\r\n        let offset = 0;\r\n        for (let i = 0; i < recBuffers.length; i++) {\r\n          result.set(recBuffers[i], offset);\r\n          offset += recBuffers[i].length;\r\n        }\r\n        return result;\r\n      }\r\n\r\n      function interleave(inputL, inputR) {\r\n        let length = inputL.length + inputR.length;\r\n        let result = new Float32Array(length);\r\n\r\n        let index = 0,\r\n          inputIndex = 0;\r\n\r\n        while (index < length) {\r\n          result[index++] = inputL[inputIndex];\r\n          result[index++] = inputR[inputIndex];\r\n          inputIndex++;\r\n        }\r\n        return result;\r\n      }\r\n\r\n      function floatTo16BitPCM(output, offset, input) {\r\n        for (let i = 0; i < input.length; i++, offset += 2) {\r\n          let s = Math.max(-1, Math.min(1, input[i]));\r\n          output.setInt16(offset, s < 0 ? s * 0x8000 : s * 0x7fff, true);\r\n        }\r\n      }\r\n\r\n      function writeString(view, offset, string) {\r\n        for (let i = 0; i < string.length; i++) {\r\n          view.setUint8(offset + i, string.charCodeAt(i));\r\n        }\r\n      }\r\n\r\n      // Down sample buffer before WAV encoding\r\n      function downSampleBuffer(buffer, rate) {\r\n        if (rate == sampleRate) {\r\n          return buffer;\r\n        }\r\n        if (rate > sampleRate) {\r\n          throw 'downsampling rate show be smaller than original sample rate';\r\n        }\r\n        var sampleRateRatio = sampleRate / rate;\r\n        var newLength = Math.round(buffer.length / sampleRateRatio);\r\n        var result = new Float32Array(newLength);\r\n        var offsetResult = 0;\r\n        var offsetBuffer = 0;\r\n        while (offsetResult < result.length) {\r\n          var nextOffsetBuffer = Math.round(\r\n            (offsetResult + 1) * sampleRateRatio\r\n          );\r\n          // Use average value of skipped samples\r\n          var accum = 0,\r\n            count = 0;\r\n          for (\r\n            var i = offsetBuffer;\r\n            i < nextOffsetBuffer && i < buffer.length;\r\n            i++\r\n          ) {\r\n            accum += buffer[i];\r\n            count++;\r\n          }\r\n          result[offsetResult] = accum / count;\r\n          // Or you can simply get rid of the skipped samples:\r\n          // result[offsetResult] = buffer[nextOffsetBuffer];\r\n          offsetResult++;\r\n          offsetBuffer = nextOffsetBuffer;\r\n        }\r\n        return result;\r\n      }\r\n\r\n      function encodeWAV(samples) {\r\n        let buffer = new ArrayBuffer(44 + samples.length * 2);\r\n        let view = new DataView(buffer);\r\n\r\n        /* RIFF identifier */\r\n        writeString(view, 0, 'RIFF');\r\n        /* RIFF chunk length */\r\n        view.setUint32(4, 36 + samples.length * 2, true);\r\n        /* RIFF type */\r\n        writeString(view, 8, 'WAVE');\r\n        /* format chunk identifier */\r\n        writeString(view, 12, 'fmt ');\r\n        /* format chunk length */\r\n        view.setUint32(16, 16, true);\r\n        /* sample format (raw) */\r\n        view.setUint16(20, 1, true);\r\n        /* channel count */\r\n        view.setUint16(22, numChannels, true);\r\n        /* sample rate */\r\n        view.setUint32(24, newSampleRate, true);\r\n        /* byte rate (sample rate * block align) */\r\n        view.setUint32(28, newSampleRate * 4, true);\r\n        /* block align (channel count * bytes per sample) */\r\n        view.setUint16(32, numChannels * 2, true);\r\n        /* bits per sample */\r\n        view.setUint16(34, 16, true);\r\n        /* data chunk identifier */\r\n        writeString(view, 36, 'data');\r\n        /* data chunk length */\r\n        view.setUint32(40, samples.length * 2, true);\r\n\r\n        floatTo16BitPCM(view, 44, samples);\r\n\r\n        return view;\r\n      }\r\n    }, self);\r\n\r\n    this.worker.postMessage({\r\n      command: 'init',\r\n      config: {\r\n        sampleRate: this.context.sampleRate,\r\n        numChannels: this.config.numChannels\r\n      }\r\n    });\r\n\r\n    this.worker.onmessage = (e) => {\r\n      let cb = this.callbacks[e.data.command].pop();\r\n      if (typeof cb == 'function') {\r\n        cb(e.data.data);\r\n      }\r\n    };\r\n  }\r\n\r\n  record() {\r\n    this.recording = true;\r\n  }\r\n\r\n  stop() {\r\n    this.recording = false;\r\n  }\r\n\r\n  clear() {\r\n    this.worker.postMessage({ command: 'clear' });\r\n  }\r\n\r\n  getBuffer(cb) {\r\n    cb = cb || this.config.callback;\r\n    if (!cb) throw new Error('Callback not set');\r\n\r\n    this.callbacks.getBuffer.push(cb);\r\n\r\n    this.worker.postMessage({ command: 'getBuffer' });\r\n  }\r\n\r\n  exportWAV(cb, mimeType) {\r\n    mimeType = mimeType || this.config.mimeType;\r\n    cb = cb || this.config.callback;\r\n    if (!cb) throw new Error('Callback not set');\r\n\r\n    this.callbacks.exportWAV.push(cb);\r\n\r\n    this.worker.postMessage({\r\n      command: 'exportWAV',\r\n      type: mimeType\r\n    });\r\n  }\r\n\r\n  static forceDownload(blob, filename) {\r\n    let url = (window.URL || window.webkitURL).createObjectURL(blob);\r\n    let link = window.document.createElement('a');\r\n    link.href = url;\r\n    link.download = filename || 'output.wav';\r\n    let click = document.createEvent('Event');\r\n    click.initEvent('click', true, true);\r\n    link.dispatchEvent(click);\r\n  }\r\n}\r\n\r\nexport default Recorder;\r\n",["36","37"],{"ruleId":"38","replacedBy":"39"},{"ruleId":"40","replacedBy":"41"},{"ruleId":"38","replacedBy":"39"},{"ruleId":"40","replacedBy":"41"},{"ruleId":"42","severity":1,"message":"43","line":99,"column":6,"nodeType":"44","endLine":99,"endColumn":8,"suggestions":"45"},{"ruleId":"46","severity":1,"message":"47","line":348,"column":10,"nodeType":"48","messageId":"49","endLine":348,"endColumn":36},{"ruleId":"50","severity":1,"message":"51","line":50,"column":9,"nodeType":"52","messageId":"53","endLine":66,"endColumn":10},{"ruleId":"54","severity":1,"message":"55","line":171,"column":18,"nodeType":"56","messageId":"57","endLine":171,"endColumn":20},{"ruleId":"58","severity":1,"message":"59","line":175,"column":11,"nodeType":"60","messageId":"61","endLine":175,"endColumn":79},{"ruleId":"38","replacedBy":"62"},{"ruleId":"40","replacedBy":"63"},"no-native-reassign",["64"],"no-negated-in-lhs",["65"],"react-hooks/exhaustive-deps","React Hook useEffect has missing dependencies: 'crossBrowser', 'googleApiKey', and 'useOnlyGoogleCloud'. Either include them or remove the dependency array.","ArrayExpression",["66"],"@typescript-eslint/no-unused-vars","'rawKeywordsContextsElement' is assigned a value but never used.","Identifier","unusedVar","default-case","Expected a default case.","SwitchStatement","missingDefaultCase","eqeqeq","Expected '===' and instead saw '=='.","BinaryExpression","unexpected","no-throw-literal","Expected an error object to be thrown.","ThrowStatement","object",["64"],["65"],"no-global-assign","no-unsafe-negation",{"desc":"67","fix":"68"},"Update the dependencies array to be: [crossBrowser, googleApiKey, useOnlyGoogleCloud]",{"range":"69","text":"70"},[3234,3236],"[crossBrowser, googleApiKey, useOnlyGoogleCloud]"]